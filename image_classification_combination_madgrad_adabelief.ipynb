{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "IZ2s6PmzSm_n",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Combined version of Madgrad and AdaBelief for Image Classification on CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX9pvQdrjwAo"
      },
      "source": [
        "### Import relevant librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKZqL0joSm_p",
        "outputId": "8b2b6a62-0ae5-48e3-b5b7-66e75e7862f6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: madgrad in /usr/local/lib/python3.7/dist-packages (1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: adabelief-pytorch==0.2.0 in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.7/dist-packages (from adabelief-pytorch==0.2.0) (0.8.9)\n",
            "Requirement already satisfied: colorama>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from adabelief-pytorch==0.2.0) (0.4.4)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from adabelief-pytorch==0.2.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (4.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.10.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.8.0)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.37)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.2.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.7.1)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (0.5.1)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.4.1)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.9.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.2.0)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install madgrad\n",
        "! pip install adabelief-pytorch==0.2.0\n",
        "! pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0fTS3UhZSm_r",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from adabelief_pytorch import AdaBelief\n",
        "from madgrad import MADGRAD\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxdfD75cSm_r",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Link to the Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVAD3KNsTFTS",
        "outputId": "a559b192-a927-40ff-84a6-feb1bb293106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/EPFL/OptML/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_yOs6IEjwAt"
      },
      "source": [
        "### Configure the GPU\n",
        "\n",
        "The second line is only valid if a GPU is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ezXVTvQ3Sm_s",
        "outputId": "c152319e-7391-43a4-b808-d3762d896fac",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGhr9QEsSm_t",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Load CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DchoKxw-Sm_t",
        "outputId": "b9cecf62-c7e9-4a89-ecc3-adde1cd0d555",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Transforamtion of the datasets into normalized tensors\n",
        "transform = transforms.Compose(\n",
        "[transforms.ToTensor(),\n",
        " transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset= torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testset= torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHIz0UN3jwAw"
      },
      "source": [
        "### Define the model and train it\n",
        "\n",
        "In this section, we define the model and we train it for 5 different seeds. At the end of each seed, we save the data in the drive.\n",
        "\n",
        "Here we change the optimizer from Madgrad to AdaBelief at switching time T."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ9M0rpgSm_y",
        "outputId": "207f0967-2bbb-4843-f2dd-058b2cb27fbb",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/100, Loss: 1.3803\n",
            "Epoch 2/100, Loss: 0.6371\n",
            "Epoch 3/100, Loss: 0.9246\n",
            "Epoch 4/100, Loss: 0.6219\n",
            "Epoch 5/100, Loss: 0.7822\n",
            "Epoch 6/100, Loss: 0.5588\n",
            "Epoch 7/100, Loss: 0.5485\n",
            "Epoch 8/100, Loss: 0.4295\n",
            "Epoch 9/100, Loss: 0.6224\n",
            "Epoch 10/100, Loss: 0.3011\n",
            "Epoch 11/100, Loss: 0.3463\n",
            "Epoch 12/100, Loss: 0.5159\n",
            "Epoch 13/100, Loss: 0.2475\n",
            "Epoch 14/100, Loss: 0.1441\n",
            "Epoch 15/100, Loss: 0.1788\n",
            "Epoch 16/100, Loss: 0.2145\n",
            "Epoch 17/100, Loss: 0.1242\n",
            "Epoch 18/100, Loss: 0.1352\n",
            "Epoch 19/100, Loss: 0.0555\n",
            "Epoch 20/100, Loss: 0.2055\n",
            "Epoch 21/100, Loss: 0.1121\n",
            "Epoch 22/100, Loss: 0.1489\n",
            "Epoch 23/100, Loss: 0.0659\n",
            "Epoch 24/100, Loss: 0.0781\n",
            "Epoch 25/100, Loss: 0.0477\n",
            "Epoch 26/100, Loss: 0.1535\n",
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  ---------\n",
            "adabelief-pytorch=0.0.5  1e-08  False              False\n",
            ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "Epoch 27/100, Loss: 0.0666\n",
            "Epoch 28/100, Loss: 0.0300\n",
            "Epoch 29/100, Loss: 0.1013\n",
            "Epoch 30/100, Loss: 0.0312\n",
            "Epoch 31/100, Loss: 0.0357\n",
            "Epoch 32/100, Loss: 0.0108\n",
            "Epoch 33/100, Loss: 0.0827\n",
            "Epoch 34/100, Loss: 0.0427\n",
            "Epoch 35/100, Loss: 0.0799\n",
            "Epoch 36/100, Loss: 0.0038\n",
            "Epoch 37/100, Loss: 0.0242\n",
            "Epoch 38/100, Loss: 0.1963\n",
            "Epoch 39/100, Loss: 0.0215\n",
            "Epoch 40/100, Loss: 0.0446\n",
            "Epoch 41/100, Loss: 0.0130\n",
            "Epoch 42/100, Loss: 0.0563\n",
            "Epoch 43/100, Loss: 0.0907\n",
            "Epoch 44/100, Loss: 0.0663\n",
            "Epoch 45/100, Loss: 0.0749\n",
            "Epoch 46/100, Loss: 0.0075\n",
            "Epoch 47/100, Loss: 0.1465\n",
            "Epoch 48/100, Loss: 0.0137\n",
            "Epoch 49/100, Loss: 0.0102\n",
            "Epoch 50/100, Loss: 0.0862\n",
            "Epoch 51/100, Loss: 0.0397\n",
            "Epoch 52/100, Loss: 0.0055\n",
            "Epoch 53/100, Loss: 0.0439\n",
            "Epoch 54/100, Loss: 0.0044\n",
            "Epoch 55/100, Loss: 0.0549\n",
            "Epoch 56/100, Loss: 0.0620\n",
            "Epoch 57/100, Loss: 0.0414\n",
            "Epoch 58/100, Loss: 0.0021\n",
            "Epoch 59/100, Loss: 0.0149\n",
            "Epoch 60/100, Loss: 0.1230\n",
            "Epoch 61/100, Loss: 0.0060\n",
            "Epoch 62/100, Loss: 0.0593\n",
            "Epoch 63/100, Loss: 0.0088\n",
            "Epoch 64/100, Loss: 0.0149\n",
            "Epoch 65/100, Loss: 0.0114\n",
            "Epoch 66/100, Loss: 0.0051\n",
            "Epoch 67/100, Loss: 0.0722\n",
            "Epoch 68/100, Loss: 0.0725\n",
            "Epoch 69/100, Loss: 0.0207\n",
            "Epoch 70/100, Loss: 0.1252\n",
            "Epoch 71/100, Loss: 0.0051\n",
            "Epoch 72/100, Loss: 0.0028\n",
            "Epoch 73/100, Loss: 0.0797\n",
            "Epoch 74/100, Loss: 0.0076\n",
            "Epoch 75/100, Loss: 0.0846\n",
            "Epoch 76/100, Loss: 0.0013\n",
            "Epoch 77/100, Loss: 0.0203\n",
            "Epoch 78/100, Loss: 0.0114\n",
            "Epoch 79/100, Loss: 0.0016\n",
            "Epoch 80/100, Loss: 0.0055\n",
            "Epoch 81/100, Loss: 0.0081\n",
            "Epoch 82/100, Loss: 0.0148\n",
            "Epoch 83/100, Loss: 0.0072\n",
            "Epoch 84/100, Loss: 0.0008\n",
            "Epoch 85/100, Loss: 0.0083\n",
            "Epoch 86/100, Loss: 0.0069\n",
            "Epoch 87/100, Loss: 0.0088\n",
            "Epoch 88/100, Loss: 0.0176\n",
            "Epoch 89/100, Loss: 0.0043\n",
            "Epoch 90/100, Loss: 0.0087\n",
            "Epoch 91/100, Loss: 0.0131\n",
            "Epoch 92/100, Loss: 0.0135\n",
            "Epoch 93/100, Loss: 0.0159\n",
            "Epoch 94/100, Loss: 0.0061\n",
            "Epoch 95/100, Loss: 0.0761\n",
            "Epoch 96/100, Loss: 0.0032\n",
            "Epoch 97/100, Loss: 0.0090\n",
            "Epoch 98/100, Loss: 0.0043\n",
            "Epoch 99/100, Loss: 0.0108\n",
            "Epoch 100/100, Loss: 0.0002\n",
            "1914.9775159358978\n",
            "Accuracy on the test images: 77.56 %\n",
            "0.024651827431856142\n"
          ]
        }
      ],
      "source": [
        "# Run the training 5 times\n",
        "for i in range(5):\n",
        "    print(i) \n",
        "    start_time = time.time()\n",
        "    torch.manual_seed(i)\n",
        "    \n",
        "    # Define the ResNet18 model \n",
        "    model = models.resnet18()\n",
        "\n",
        "    # Define the loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Hyper-parameter for the switching time\n",
        "    T = 26\n",
        "\n",
        "    # Configure the optimizer with default hyperparameters\n",
        "    optimizer = MADGRAD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0, eps=1e-6, decouple_decay=False)\n",
        "    \n",
        "    model.to(device)\n",
        "    num_epochs = 100\n",
        "    total_step = len(trainloader)\n",
        "    loss_trial = []\n",
        "    acc_trial = []\n",
        "    \n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Check if we switch the optimizer to AdaBelief\n",
        "        if epoch == T :\n",
        "            optimizer = AdaBelief(model.parameters(), lr=0.001, betas=(0.9,0.999), eps=1e-16, weight_decouple=False, rectify=False)\n",
        "\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        \n",
        "        # Training for one epoch\n",
        "        for j, (images, labels) in enumerate(trainloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Set the parameter gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Backward and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print (\"Epoch {}/{}, Loss: {:.4f}\"\n",
        "                      .format(epoch+1, num_epochs, loss.item()))\n",
        "        \n",
        "        # Store the loss at the end of the epoch\n",
        "        loss_trial.append(total_loss/total_step)\n",
        "\n",
        "        # Test the model at the end of each epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            # Count the number of good predictions\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in testloader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                # Check if the prediction matches the exact label value\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Store the accuracy at the end of the epoch\n",
        "        acc_trial.append(100 * correct/total)\n",
        "\n",
        "    # Total time for the training\n",
        "    train_time = time.time() - start_time\n",
        "    \n",
        "    # Print relevant results\n",
        "    print(train_time)\n",
        "    print('Accuracy on the test images: {} %'.format(100 * correct / total))\n",
        "    print(loss_trial[-1])\n",
        "    \n",
        "    # Save the results externally\n",
        "    np.save('/content/drive/MyDrive/EPFL/OptML/loss_resnet18_combined_T{}_episode_{}.npy'.format(T, i), loss_trial)\n",
        "    np.save('/content/drive/MyDrive/EPFL/OptML/acc_resnet18_combined_T{}_episode_{}.npy'.format(T, i), acc_trial)\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "image_classification_combination_madgrad_adabelief.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}